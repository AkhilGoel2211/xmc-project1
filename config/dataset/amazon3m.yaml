data:
  dataset: amazon3m
  is_lf_data: False
  augment_label_data: False
  use_filter_eval: False
  num_labels: 2812281 
  max_len: 128
  num_workers: 8
  batch_size: 128
  test_batch_size: 128

#Architectural Settings
model:
  encoder:
    encoder_model: "bert-base-uncased"
    encoder_tokenizer: "${dataset.model.encoder.encoder_model}"
    encoder_ftr_dim: ${encoder_feature_size:${dataset.model.encoder.encoder_model}}
    pool_mode: 'last_nhidden_conlast' 
    feature_layers: 1
    embed_dropout: 0.35
    use_torch_compile: False
    use_ngame_encoder_weights: False
    ngame_checkpoint: ./NGAME_ENCODERS/${dataset.data.dataset}/state_dict.pt

  penultimate:
    use_penultimate_layer: False
    penultimate_size: 128
    penultimate_activation: relu

  ffi:
    use_sparse_layer: True
    fan_in: 128
    prune_mode: fraction
    rewire_threshold: 0.01
    rewire_fraction: 0.25
    growth_mode: random
    growth_init_mode: zero
    input_features: ${input_size_select:${dataset.model.penultimate.use_penultimate_layer},
                          ${dataset.model.penultimate.penultimate_size},${dataset.model.encoder.feature_layers},${dataset.model.encoder.encoder_ftr_dim}}   #set based on penultimate value in Encoder
    output_features: ${dataset.data.num_labels} #depends on num_labels in data
    rewire_interval: 1500
    use_rewire_scheduling: True
    rewire_end_epoch: 0.66   #depends on epoch

  auxiliary:
    use_meta_branch: False
    group_y_group: 0
    meta_cutoff_epoch: 5   # varies based on fan_in values
    auxloss_scaling: 0.5

#Training related settings
training:
  seed: 42
  amp:
    enabled: True
    dtype: float16

  optimization:
    loss_fn: squared_hinge   # ['bce','squared_hinge']
    encoder_optimizer: adamw
    xmc_optimizer: adamw
    epochs: 51  
    grad_accum_step: 2
    encoder_lr: 5.0e-5
    penultimate_lr: 2.0e-4
    meta_lr: 5.0e-4
    lr: 0.05  # learning rate of final layer
    wd_encoder: 0.01   # weight decay on encoder
    wd: 1e-4  # weight decay of final layer
    lr_scheduler: CosineScheduleWithWarmup   
    lr_scheduler_xmc: CosineScheduleWithWarmup 
    warmup_steps: 10000
    training_steps: 1  #selected at runtime based on batch size and dataloader

  evaluation:
    train_evaluate: False
    train_evaluate_every: 10
    test_evaluate_every: 1
    A: 0.6  # for propensity calculation
    B: 2.6  # for propensity calculation
    eval_psp: True

  verbose:
    show_iter: False  # print loss during training
    print_iter: 2000  # how often (iteration) to print
    use_wandb: False
    wandb_runname: none
    logging: True
    log_fname: log_amazon3m
    
  use_checkpoint: False  #whether to use automatic checkpoint
  checkpoint_file: amazon3m.pt
  best_p1: 0.48  # to store the model above this performance in case of automatic checkpoint
